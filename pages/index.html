<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Semanticist: PCA-Guided Visual Tokenization</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 40px;
            line-height: 1.6;
            max-width: 950px;
            margin: auto;
        }
        h1, h2, h3 {
            color: #333;
        }
        a {
            color: #007bff;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        .section {
            margin-bottom: 40px;
        }
        .author_section {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 80%;
            text-align: center;
        }
        img {
            max-width: 100%;
            max-height: 100%;
        }
        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 50%;
        }
        table {
            width: 60%;
            border-collapse: collapse;
            background: #ffffff;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            border-radius: 8px;
            overflow: hidden;
        }

        th, td {
            padding: 12px 16px;
            text-align: left;
        }

        th {
            background: #f4f4f4;
            font-weight: bold;
        }

        tr {
            border-bottom: 1px solid #ddd;
        }

        tr:last-child {
            border-bottom: none;
        }

        tr:hover {
            background: #f1f1f1;
        }
    </style>
</head>
<body>
    <h1>"Principal Components" Enable A New Language of Images</h1>
    <div class="author_section">
    <h3>(ICCV 2025) A New Paradigm for Compact and Interpretable Image Representations</h3>
    <p>
        <a href="https://arxiv.org/abs/2503.08685">[Read the Paper]</a> &nbsp; | &nbsp;
        <a href="https://github.com/visual-gen/semanticist">[GitHub]</a> &nbsp; | &nbsp;
        <a href="https://huggingface.co/spaces/tennant/semanticist_tokenizer">[Huggingface Tokenizer Demo]</a> &nbsp; | &nbsp;
        <a href="https://huggingface.co/spaces/tennant/Semanticist_AR">[Huggingface Generation Demo]</a>
    </p>
        <p>
            <a href="https://wen-xin.info/">Xin Wen</a><sup>1*</sup>, 
            <a href="https://bzhao.me/">Bingchen Zhao</a><sup>2*</sup>, 
            <a href="https://therevanchist.github.io/">Ismail Elezi</a><sup>3</sup>, 
            <a href="https://jiankangdeng.github.io/">Jiankang Deng</a><sup>4</sup>, 
            <a href="https://xjqi.github.io/">Xiaojuan Qi</a><sup>1</sup>
        <br/>
        <small><sup>*</sup> Equal Contribution &nbsp;</small>
        <br/>
            <sup>1</sup> University of Hong Kong &nbsp; | &nbsp;
            <sup>2</sup> University of Edinburgh <br/>
            <sup>3</sup> Huawei London Research Centre &nbsp; | &nbsp;
            <sup>4</sup> Imperial College London
        </p>
    </div>
    <div class="section">
        <div class="img">
            <img src="figs/teaser.jpg" alt="Semanticist Teaser" >
        </div>
    </div>


    <div class="section">
        <h2>Introduction & Motivation</h2>
        <p>Deep generative models have revolutionized image synthesis, but how we tokenize visual data remains an open question. 
        While classical methods like <b>Principal Component Analysis (PCA)</b> introduced compact, structured representations, modern <b>visual tokenizers</b>‚Äîfrom <b>VQ-VAE</b> to <b>SD-VAE</b>‚Äîoften prioritize <b>reconstruction fidelity</b> at the cost of interpretability and efficiency.</p>
        <h3>The Problem</h3>
        <ul>
            <li><b>Lack of Structure:</b> Tokens are arbitrarily learned, without an ordering that prioritizes important visual features first.</li>
            <li><b>Semantic-Spectrum Coupling:</b> Tokens entangle <i>high-level semantics</i> with <i>low-level spectral details</i>, leading to inefficiencies in downstream applications.</li>
        </ul>
        <p>Can we design a <b>compact, structured tokenizer</b> that retains the benefits of PCA while leveraging modern generative techniques?</p>
    </div>

    <div class="section">
        <h2>Demo</h2>
        <h3>Semanticist Tokenizer Demo</h3>
        <div style="margin-bottom: 20px;">
            <iframe
                src="https://tennant-semanticist-tokenizer.hf.space"
                frameborder="0"
                width="850"
                height="750"
            ></iframe>
        </div>
        <h3>Semanticist AR Generation Demo</h3>
        <div>
            <iframe
                src="https://tennant-semanticist-ar.hf.space"
                frameborder="0"
                width="850"
                height="750"
            ></iframe>
        </div>
    </div>

    <div class="section">
        <h2>Key Contributions (What‚Äôs New?)</h2>
        <ul>
            <li><b>üìå PCA-Guided Tokenization:</b> Introduces a <i>causal ordering</i> where earlier tokens capture the most important visual details, reducing redundancy.</li>
            <li><b>‚ö° Semantic-Spectrum Decoupling:</b> Resolves the issue of semantic-spectrum coupling to ensure tokens focus on high-level semantic information.</li>
            <li><b>üåÄ Diffusion-Based Decoding:</b> Uses a <i>diffusion decoder</i> for the spectral auto-regressive property to naturally separate semantic and spectral content.</li>
            <li><b>üöÄ Compact & Interpretability-Friendly:</b> Enables <i>flexible token selection</i>, where fewer tokens can still yield high-quality reconstructions.</li>
        </ul>
    </div>

    <div class="section">
        <h2>Visualizing the Problem: Semantic-Spectrum Coupling</h2>
        <p>Existing methods fail to separate <b>semantics from spectral details</b>, leading to inefficiencies in token usage.</p>
        <ul>
            <li><b>üö® Current Tokenizers:</b> More tokens simultaneously increase both <i>semantic content</i> and <i>low-level spectral details</i>, making compression inefficient.</li>
            <li><b>‚úÖ Our Approach:</b> Tokens capture <i>semantics first</i>, ensuring a <i>coarse-to-fine</i> hierarchical structure.</li>
        </ul>
        <!-- <p><b>üñº Comparison of Reconstructions</b><br>Ô∏è</p>  -->
        <p><b>üìä Power Spectrum Analysis (Visual)</b><br></p>
        <div class="img">
            <img src="figs/spectral_titok_ours.jpg">
        </div>
        <p>This figure illustrates the <b>semantic-spectrum coupling effect</b> by comparing reconstructions from <b>TiTok</b> (top) and <b>our method (bottom)</b> using an increasing number of tokens.</p>

        <ul>
            <li><b>Top Row (TiTok):</b>
                <ul>
                    <li>As more tokens are added, <b>both semantic details and spectral power increase simultaneously</b>.</li>
                    <li>The power spectrum (red: GT, blue: reconstructed) shifts upward, showing spectral entanglement.</li>
                    <li>Earlier reconstructions fail to preserve semantic meaningful details.</li>
                </ul>
            </li>
        </ul>

        <ul>
            <li><b>Bottom Row (Ours - Semanticist):</b>
                <ul>
                    <li>Our method maintains <b>semantic clarity</b> even with fewer tokens.</li>
                    <li>The power spectrum remains <b>consistent</b> with the original image across different token counts, confirming <b>spectral decoupling</b>.</li>
                    <li>Reconstructions follow a <b>coarse-to-fine hierarchy</b>, mirroring the <b>global precedence effect</b> in human vision.</li>
                </ul>
            </li>
        </ul>

        <p>This analysis demonstrates that <b>Semanticist produces a structured latent space</b> where tokens capture high-level semantic meaning first, avoiding spectral artifacts. The below figure gives more comparisons.</p>

        <div class="img">
            <img src="figs/spectral_analysis.png">
        </div>
    </div>

    <div class="section">
        <h2>Model Architecture: Structured Visual Tokenization with a PCA-like Hierarchy</h1>

            <p>Our model introduces a structured <strong>1D causal tokenization</strong> framework designed to efficiently encode images into a compact and semantically meaningful latent space. Unlike conventional tokenizers that encode images into a <strong>2D grid of latent vectors</strong>, our approach enforces a <strong>hierarchical PCA-like structure</strong>, where each token progressively refines the image representation in a coarse-to-fine manner.</p>

            <h2>Key Components of Our Architecture</h2>

            <h3>1. Causal ViT Encoder</h3>
            <p>The encoding process begins with a <strong>Causal Vision Transformer (ViT) Encoder</strong>, which receives an input image and generates <strong>concept tokens</strong> in a 1D sequence. Unlike conventional <strong>2D latent spaces</strong>, these tokens are ordered <strong>causally</strong>, ensuring that earlier tokens capture the most salient semantic features, while later tokens refine details.</p>
            <p>üëâ See the figure below, where the encoder transforms the input image into a structured token sequence.</p>
                <img src="figs/tokenizer.png" alt="Causal ViT Encoder Diagram" width="50%" class="center">

            <h3>2. Nested Classifier-Free Guidance (CFG) for PCA-like Structure</h3>
            <p>To enforce a PCA-like hierarchy, we apply a <strong>nested classifier-free guidance (CFG) strategy</strong>, where later tokens are progressively replaced with a <strong>null condition token</strong> during training. This forces earlier tokens to <strong>prioritize capturing the most critical information</strong>, leading to an interpretable, structured representation.</p>
            <p>üëâ The image above illustrates how nested CFG selectively refines token importance.</p>
            <p>üìö This PCA-like structure is mathematically proved in our paper.</p>

            <h3>3. Diffusion-Based DiT Decoder</h3>
            <p>A <strong>Diffusion Transformer (DiT) Decoder</strong> reconstructs the image from these structured latent tokens. Unlike traditional deterministic decoders, our diffusion-based approach <strong>naturally follows a spectral autoregressive process</strong>, reconstructing images from low to high frequencies. This prevents <strong>semantic-spectrum coupling</strong>, ensuring that tokens encode high-level meaning instead of low-level artifacts.</p>
            <p>üëâ The figure below demonstrates how image reconstructions progressively improve as more tokens are used.</p>
                <img src="figs/Token_PCA.png" alt="Token Reconstruction Diagram" width="50%" class="center">

            <h2>Coarse-to-Fine Token Representation</h2>
            <p>Our hierarchical tokenization closely resembles the <strong>global precedence effect</strong> in human vision, where broader structures are perceived before finer details. This property allows our tokenizer to <strong>adaptively reconstruct images</strong> with varying numbers of tokens, making it highly flexible for <strong>compression, image generation, and recognition tasks</strong>.</p>
            <p>üëâ As shown in the image above, increasing the number of tokens leads to progressively better reconstructions while maintaining structured information.</p>

            <h2>Why Our Model is Different</h2>
            <ul>
                <li><strong>‚úî 1D Causal Tokenization</strong> ‚Äì Unlike 2D token grids, our model enforces an ordered structure where token importance follows a hierarchical pattern.</li>
                <li><strong>‚úî PCA-Like Variance Decay</strong> ‚Äì Earlier tokens contain the most significant information, while later tokens refine details, mimicking PCA decomposition.</li>
                <li><strong>‚úî Diffusion-Based Decoding</strong> ‚Äì Prevents <strong>semantic-spectrum entanglement</strong>, ensuring that tokens capture high-level meaning rather than low-level frequency artifacts.</li>
            </ul>
    </div>

    <div class="section">
        <h2>Experimental Results</h2>
        <p>We validate <b>Semanticist</b> through extensive experiments, demonstrating:</p>
        <ul>
            <li><b>üèÜ State-of-the-art Reconstruction:</b> Achieves the lowest FID scores among visual tokenizers.</li>
            <li><b>üé® Better Generative Performance:</b> Autoregressive models trained on Semanticist tokens match leading baselines with fewer tokens.</li>
            <li><b>üìà Improved Interpretability:</b> PCA-like hierarchy aligns with human perception and enhances linear probing classification accuracy.</li>
        </ul>
        <p><b>üìù Quantitative Results Table</b><br></p> 
        <div class="img">
            <img src="figs/comp_table.jpg">
        </div>
        <!-- <table border="1">
            <thead>
                <tr>
                    <th>Method</th>
                    <th>#Token</th>
                    <th>Dim.</th>
                    <th>VQ</th>
                    <th>rFID‚Üì</th>
                    <th>PSNR‚Üë</th>
                    <th>SSIM‚Üë</th>
                    <th>Gen. Model</th>
                    <th>Type</th>
                    <th>#Token</th>
                    <th>#Step</th>
                    <th>gFID‚Üì</th>
                    <th>IS‚Üë</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>MaskBit</td>
                    <td>256</td>
                    <td>12</td>
                    <td>‚úî</td>
                    <td>1.61</td>
                    <td>--</td>
                    <td>--</td>
                    <td>MaskBit</td>
                    <td>Mask.</td>
                    <td>256</td>
                    <td>256</td>
                    <td>1.52</td>
                    <td>328.6</td>
                </tr>
                <tr>
                    <td>RCG (cond.)</td>
                    <td>1</td>
                    <td>256</td>
                    <td>‚úò</td>
                    <td>--</td>
                    <td>--</td>
                    <td>--</td>
                    <td>MAGE-L</td>
                    <td>Mask.</td>
                    <td>1</td>
                    <td>20</td>
                    <td>3.49</td>
                    <td>215.5</td>
                </tr>
                <tr>
                    <td>MAR</td>
                    <td>256</td>
                    <td>16</td>
                    <td>‚úò</td>
                    <td>1.22</td>
                    <td>--</td>
                    <td>--</td>
                    <td>MAR-L</td>
                    <td>Mask.</td>
                    <td>256</td>
                    <td>64</td>
                    <td>1.78</td>
                    <td>296.0</td>
                </tr>
                <tr>
                    <td>TiTok-S-128</td>
                    <td>128</td>
                    <td>16</td>
                    <td>‚úî</td>
                    <td>1.71</td>
                    <td>--</td>
                    <td>--</td>
                    <td>MaskGIT-L</td>
                    <td>Mask.</td>
                    <td>128</td>
                    <td>64</td>
                    <td>1.97</td>
                    <td>281.8</td>
                </tr>
                <tr>
                    <td>TiTok-L-32</td>
                    <td>32</td>
                    <td>8</td>
                    <td>‚úî</td>
                    <td>2.21</td>
                    <td>--</td>
                    <td>--</td>
                    <td>MaskGIT-L</td>
                    <td>Mask.</td>
                    <td>32</td>
                    <td>8</td>
                    <td>2.77</td>
                    <td>194.0</td>
                </tr>
                <tr>
                    <td>VQGAN</td>
                    <td>256</td>
                    <td>16</td>
                    <td>‚úî</td>
                    <td>7.94</td>
                    <td>--</td>
                    <td>--</td>
                    <td>Tam. Trans.</td>
                    <td>AR</td>
                    <td>256</td>
                    <td>256</td>
                    <td>5.20</td>
                    <td>280.3</td>
                </tr>
                <tr>
                    <td>ViT-VQGAN</td>
                    <td>1024</td>
                    <td>32</td>
                    <td>‚úî</td>
                    <td>1.28</td>
                    <td>--</td>
                    <td>--</td>
                    <td>VIM-L</td>
                    <td>AR</td>
                    <td>1024</td>
                    <td>1024</td>
                    <td>4.17</td>
                    <td>175.1</td>
                </tr>
                <tr>
                    <td>RQ-VAE</td>
                    <td>256</td>
                    <td>256</td>
                    <td>‚úî</td>
                    <td>3.20</td>
                    <td>--</td>
                    <td>--</td>
                    <td>RQ-Trans.</td>
                    <td>AR</td>
                    <td>256</td>
                    <td>64</td>
                    <td>3.80</td>
                    <td>323.7</td>
                </tr>
                <tr>
                    <td>VAR</td>
                    <td>680</td>
                    <td>32</td>
                    <td>‚úî</td>
                    <td>0.90</td>
                    <td>--</td>
                    <td>--</td>
                    <td>VAR-$d$16</td>
                    <td>VAR</td>
                    <td>680</td>
                    <td>10</td>
                    <td>3.30</td>
                    <td>274.4</td>
                </tr>
                <tr>
                    <td>ImageFolder</td>
                    <td>286</td>
                    <td>32</td>
                    <td>‚úî</td>
                    <td>0.80</td>
                    <td>--</td>
                    <td>--</td>
                    <td>VAR-$d$16</td>
                    <td>VAR</td>
                    <td>286</td>
                    <td>10</td>
                    <td>2.60</td>
                    <td>295.0</td>
                </tr>
                <tr>
                    <td>LlamaGen</td>
                    <td>256</td>
                    <td>8</td>
                    <td>‚úî</td>
                    <td>2.19</td>
                    <td>20.79</td>
                    <td>0.675</td>
                    <td>LlamaGen-L</td>
                    <td>AR</td>
                    <td>256</td>
                    <td>256</td>
                    <td>3.80</td>
                    <td>248.3</td>
                </tr>
                <tr>
                    <td>CRT</td>
                    <td>256</td>
                    <td>8</td>
                    <td>‚úî</td>
                    <td>2.36</td>
                    <td>--</td>
                    <td>--</td>
                    <td>LlamaGen-L</td>
                    <td>AR</td>
                    <td>256</td>
                    <td>256</td>
                    <td>2.75</td>
                    <td>265.2</td>
                </tr>
                <tr>
                    <td>Causal MAR</td>
                    <td>256</td>
                    <td>16</td>
                    <td>‚úò</td>
                    <td>1.22</td>
                    <td>--</td>
                    <td>--</td>
                    <td>MAR-L</td>
                    <td>AR</td>
                    <td>256</td>
                    <td>256</td>
                    <td>4.07</td>
                    <td>232.4</td>
                </tr>
                <tr>
                    <td><strong>Semanticist w/ DiT-L</strong></td>
                    <td>256</td>
                    <td>16</td>
                    <td>‚úò</td>
                    <td>0.78</td>
                    <td>21.61</td>
                    <td>0.626</td>
                    <td>AR</td>
                    <td>32</td>
                    <td>32</td>
                    <td>2.57</td>
                    <td>260.9</td>
                </tr>
            </tbody>
        </table> -->

    </div>

    <div class="section">
        <h2>Broader Impact & Limitations</h2>
        <h3>Potential Applications</h3>
        <ul>
            <li><b>üîé Image Compression:</b> More efficient representations with reduced redundancy.</li>
            <li><b>üé≠ Generative Models:</b> Enhanced image synthesis with structured latents.</li>
            <li><b>üìä Data Analysis:</b> Improved interpretability and feature extraction.</li>
        </ul>
        <h3>Limitations</h3>
        <ul>
            <li><b>‚è≥ Inference Speed:</b> Diffusion decoding is slower than direct pixel regression.</li>
            <li><b>ü§ñ Alternative Architectures:</b> Flow-matching or consistency models could improve efficiency.</li>
            <li><b>üìâ Adaptive Tokenization:</b> Dynamic token lengths could further optimize representation.</li>
        </ul>
        <h3>Ethical Considerations</h3>
        <p>Like all generative models, our approach could be misused for deepfake creation or content manipulation. We encourage responsible use and propose safeguards to mitigate misuse.</p>
    </div>

    <div class="section">
        <h3>Acknowledgements</h3>
        <p>We sincerely appreciate the dedicated support we received from the participants of the human study. We are also grateful to Anlin Zheng and Haochen Wang for helpful suggestions on the design of technical details.</p>
        <h3>Author Contribution Statement</h3>
        <p>X.W. and B.Z. conceived the study and guided its overall direction and planning.
            X.W. proposed the original idea of semantically meaningful decomposition for image tokenization.
            B.Z. developed the theoretical framework for nested CFG and the semantic spectrum coupling effect and conducted the initial feasibility experiments.
            X.W. further refined the model architecture and scaled the study to ImageNet.
            B.Z. led the initial draft writing, while X.W. designed the figures and plots.
            I.E., J.D., and X.Q. provided valuable feedback on the manuscript.
            All authors contributed critical feedback, shaping the research, analysis, and final manuscript.
            </p>
    </div>

    <div class="section">
        <p><b>Citation:</b> If you find our work useful, please cite us!</p>
        <pre>
    @inproceedings{semanticist,
        title={``{P}rincipal Components'' Enable A New Language of Images},
        author={Wen, Xin and Zhao, Bingchen and Elezi, Ismail and Deng, Jiankang and Qi, Xiaojuan},
        booktitle={IEEE/CVF International Conference on Computer Vision (ICCV)},
        year={2025}
    }
        </pre>
    </div>
    <div class="section">
        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=ohfF7QnX6TAiFMBG_o5tNCpihexmou-yENIu04AAlEE&cl=ffffff&w=a"></script>
    </div>
</body>
</html>
